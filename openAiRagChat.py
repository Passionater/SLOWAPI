import os
from time import time
from pymongo import MongoClient
from langchain_huggingface import HuggingFaceEmbeddings
from openai import OpenAI
from dotenv import load_dotenv
import time
import re

# ì „ì—­ ë³€ìˆ˜ ì´ˆê¸°í™”
def init_settings():
    global client, db, embedding_model, OPENAI_API_KEY
    # 1. MongoDB ì—°ê²°
    MONGO_URI = os.getenv("MONGO_URI")
    
    client = MongoClient(MONGO_URI)
    db = client["legal_db"]

    # 2. ì„ë² ë”© ëª¨ë¸ (DB ì €ì¥í•  ë•Œì™€ ë™ì¼í•˜ê²Œ ë§ì¶°ì•¼ í•¨)
    embedding_model = HuggingFaceEmbeddings(
        model_name="jhgan/ko-sroberta-multitask",
        model_kwargs={'device': 'cpu'}
    )

    # 3. OPENAI_API_KEY ì„¤ì •
    OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")


# 4. ê³µí†µ ê²€ìƒ‰ í•¨ìˆ˜
def search_collection(collection, index_name, embedding, top_k=3):
    pipeline = [
        {
            "$vectorSearch": {
                "index": index_name,
                "queryVector": embedding,
                "path": "embedding",
                "numCandidates": 100,
                "limit": top_k
            }
        },
        {
            "$project": {
                "_id": 0,
                "doc_type": collection.name,
                "case_no": 1,
                "case_name": 1,
                "law_id": 1,
                "law_name": 1,
                "holding": 1,
                "text": 1,
                "material_type": 1,
                "edition": 1,
                "org_author": 1,
                "filename": 1,
                "score": {"$meta": "vectorSearchScore"}
            }
        }
    ]
    return list(collection.aggregate(pipeline))

def get_sentences(text, max_sentences=2):
    # ë¬¸ì¥ ë‹¨ìœ„ ë¶„ë¦¬
    import re
    sentences = re.split(r'(?<=[.!?])\s+', text.strip())
    return " ".join(sentences[:max_sentences])

def clean_practice_text(text, max_sentences=3):
    import re
    # 1) ë¶ˆí•„ìš”í•œ ê³µë°± ì •ë¦¬
    text = re.sub(r'\s+', ' ', text.strip())
    
    # 2) ì•½ê´€ êµ¬ë¶„ìë¥¼ ë¬¸ì¥ ëìœ¼ë¡œ ì¸ì‹ (ì œ ì¡°, ì œ ê´€ ë“±)
    text = re.sub(r'(ì œ\s*ì¡°)', r'\1.', text)
    text = re.sub(r'(ì œ\s*ê´€)', r'\1.', text)
    
    # 3) ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„ë¦¬
    sentences = re.split(r'(?<=[.!?])\s+', text)
    
    # 4) ì•ë¶€ë¶„ ëª‡ ê°œë§Œ ê°€ì ¸ì˜¤ê¸°
    summary = " ".join(sentences[:max_sentences])
    
    return summary


def make_Context(results_cases, results_laws, results_practices):
    # ========== CASE ==========
    context = "=== ğŸ“‚ CASE ===\n"
    for r in results_cases[:3]:   # ìƒìœ„ 3ê°œë§Œ ì¶œë ¥
        if r.get("holding"):
            case_name = r.get("case_name", "ì‚¬ê±´ëª… ì—†ìŒ")
            case_no = r.get("case_no") or "ê²€ìƒ‰ìš”ë§"
            holding = get_sentences(r['holding'])
            context += f"- {case_name} (íŒë ˆë²ˆí˜¸:{case_no}): {holding}\n"

    # ========== LAW ==========
    context += "\n=== ğŸ“‚ LAW ===\n"
    if results_laws and results_laws[0].get("text"):
        r = results_laws[0]
        law_name = r.get("law_name", "ë²•ë ¹ëª… ì—†ìŒ")
        promulgation_no = r.get("promulgation_no") or "ê²€ìƒ‰ìš”ë§"
        law_text = get_sentences(r['text'])
        context += f"- {law_name} (ê³µí¬ë²ˆí˜¸:{promulgation_no}): {law_text}\n"
    else:
        context += "í•´ë‹¹ ì§ˆë¬¸ì— ì ìš©í•  ë²•ë ¹ ìë£Œê°€ ê²€ìƒ‰ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. (ì¶”ê°€ ê²€í†  í•„ìš”)"


    # ========== PRACTICE ==========
    context += "\n=== ğŸ“‚ PRACTICE ===\n"
    for r in results_practices[:3]:  # ìƒìœ„ 3ê°œë§Œ ì¶œë ¥
        if r.get("text"):
            material = r.get("material_type")
            if not material or material == "ë¯¸ìƒ":
                material = "ì•½ê´€"
            org_author = r.get("org_author") or "ì‹¤ë¬´ìë£Œ"
            filename = r.get("filename") or "ì‹¤ë¬´ìë£Œ"
            practice_text = clean_practice_text(r['text'], max_sentences=3)
            context += f"- {material}\n  Â· ì‘ì„±ì: {org_author}\n  Â· íŒŒì¼: {filename}\n  Â· ë‚´ìš©: {practice_text}\n"


    print("=== ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ ===")
    print(context)
    print("=====================")

    return context


def call_openai_api(user_prompt):
    #promptë¥¼ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜
    embedding = embedding_model.embed_query(user_prompt)

    results_cases = search_collection(db.cases, "cases_vector_index", embedding, top_k=10)
    results_laws = search_collection(db.laws, "laws_vector_index", embedding, top_k=3)
    results_practices = search_collection(db.practices, "practices_vector_index", embedding, top_k=5)

    context = make_Context(results_cases, results_laws, results_practices)

    client = OpenAI(api_key=OPENAI_API_KEY)

    # 9. í”„ë¡¬í”„íŠ¸ ìƒì„±
    prompt = f"""
    ë„ˆëŠ” ì†í•´ì‚¬ì •ì‚¬ì¸ ë™ì‹œì— ì˜¤ë«ë™ì•ˆ ì¼ í•´ì™”ê³ , ì™„ë²½íˆ ì—…ë¬´ë¥¼ ìˆ™ì§€í•˜ê³  ìˆëŠ” ë³´í—˜ ê´€ë ¨ ë²•ë¥  ì–´ì‹œìŠ¤í„´íŠ¸ë‹¤.
    ì•„ë˜ ì°¸ê³  ìë£Œë¥¼ ë¶„ì„í•˜ì—¬, ì§ˆë¬¸ê³¼ ì§ì ‘ ê´€ë ¨ëœ í•µì‹¬ ë‹µë³€ë§Œ ì œì‹œí•˜ë¼.
    - ì§ˆë¬¸ì´ ì²˜í•œ ìƒí™©ì„ ë‹¤ì‹œ í•œë²ˆ ì¸ì§€í•˜ê³  ë‹µë³€ì— ë°˜ì˜í•œë‹¤.
    - ë‹µë³€ì€ ë°˜ë“œì‹œ ì°¸ê³  ìë£Œì—ì„œ ê·¼ê±°ë¥¼ ì°¾ì•„ì•¼ í•œë‹¤.
    - ì°¸ê³  ìë£Œì™€ ë¬´ê´€í•œ ë‚´ìš©ì€ 'ì „ë¬¸ê°€ì—ê²Œ ìš”ì²­í•˜ê¸°'ë¼ê³  ë§í•œë‹¤.
    - ë‹µë³€ì€ 10ì¤„ ì´ë‚´ í™•ì‹¤í•˜ê³ , ê°„ê²°í•˜ê²Œ ìš”ì•½ë¬¸ìœ¼ë¡œ ì‘ì„±í•œë‹¤.
    - ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ì—ì„œ ì ì ˆí•œ caseì™€ practiceë¥¼ ì—°ê³„í•´ì„œ ì§ˆë¬¸ì— ì•Œ ë§ëŠ” ë‚´ìš©ë„ ë¶„ì„í•´ì„œ ìš”ì•½í•˜ê³ .
    - ì§ˆë¬¸ì´ ì²˜í•œ ìƒí™©ì„ ë‹¤ì‹œ í•œë²ˆ ì¸ì§€í•˜ê³  ë‹µë³€ì— ë°˜ì˜í•œë‹¤.
    - ì¢€ ë” ì „ë¬¸ì ì´ê³  ì„¬ì„¸í•˜ê²Œ êµ¬ì²´ì ìœ¼ë¡œ ë‹µë³€í•œë‹¤.

    ğŸ“š ì°¸ê³  ìë£Œ:
    {context}

    â“ ì§ˆë¬¸: {user_prompt}
    """
    print("ğŸš€ chat completions ì‹œì‘...")
    completions_start = time.time()
    # 10. LLM í˜¸ì¶œ
    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.2,
        max_tokens=300
    )

    completions_time = time.time() - completions_start
    print(f"\nğŸ¯ chat completions ì‹¤í–‰ ì‹œê°„: {completions_time:.2f}ì´ˆ")
    print("=" * 80)

    answer = response.choices[0].message.content.strip()
    if not answer:
        answer = "ìë£Œì— ì—†ìŒ"

    print("\n=== AI ë‹µë³€ ===")
    print(answer)

    return context ,answer


if __name__ == "__main__":

    print("ğŸš€ í”„ë¡œê·¸ë¨ ì‹œì‘...")
    total_start = time.time()
    # í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ ë° ì„¤ì • ì´ˆê¸°í™”
    load_dotenv()
    init_settings()

    # í…ŒìŠ¤íŠ¸ ì§ˆë¬¸
    user_question = "êµí†µì‚¬ê³ ë¡œ ì¸í•œ ì†í•´ë°°ìƒ ì²­êµ¬ ì†Œì†¡ì—ì„œ, ìƒëŒ€ë°©ì´ ë³´í—˜ì‚¬ì¸ ê²½ìš°ì™€ ê°œì¸ì¸ ê²½ìš°ì— ì–´ë–¤ ì°¨ì´ê°€ ìˆë‚˜ìš”?"
    context, answer = call_openai_api(user_question)

    total_program_time = time.time() - total_start
    print(f"\nğŸ¯ ì „ì²´ í”„ë¡œê·¸ë¨ ì‹¤í–‰ ì‹œê°„: {total_program_time:.2f}ì´ˆ")
    print("=" * 80)

    print(f"ì¶œë ¥ëœ context: {context}")
    print(f"ì¶œë ¥ëœ answer: {answer}")
